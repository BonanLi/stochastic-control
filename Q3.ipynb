{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd058c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9387bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGM_Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(DGM_Layer, self).__init__()\n",
    "        \n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "            \n",
    "\n",
    "        self.gate_Z = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_G = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_R = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_H = self.layer(dim_x+dim_S, dim_S)\n",
    "            \n",
    "    def layer(self, nIn, nOut):\n",
    "        l = nn.Sequential(nn.Linear(nIn, nOut), self.activation)\n",
    "        return l\n",
    "    \n",
    "    def forward(self, x, S):\n",
    "        x_S = torch.cat([x,S],1)\n",
    "        Z = self.gate_Z(x_S)\n",
    "        G = self.gate_G(x_S)\n",
    "        R = self.gate_R(x_S)\n",
    "        \n",
    "        input_gate_H = torch.cat([x, S*R],1)\n",
    "        H = self.gate_H(input_gate_H)\n",
    "        \n",
    "        output = ((1-G))*H + Z*S\n",
    "        return output\n",
    "class Net_DGM(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(Net_DGM, self).__init__()\n",
    "\n",
    "        self.dim = dim_x\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "\n",
    "        self.input_layer = nn.Sequential(nn.Linear(dim_x+1, dim_S), self.activation)\n",
    "\n",
    "        self.DGM1 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM2 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM3 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "\n",
    "        self.output_layer = nn.Linear(dim_S, 1)\n",
    "\n",
    "    def forward(self,t,x):\n",
    "        tx = torch.cat([t,x], 1)\n",
    "        S1 = self.input_layer(tx)\n",
    "        S2 = self.DGM1(tx,S1)\n",
    "        S3 = self.DGM2(tx,S2)\n",
    "        S4 = self.DGM3(tx,S3)\n",
    "        output = self.output_layer(S4)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x = 2  # 1 dimension for time + 2 dimensions for space\n",
    "dim_S = 100  # Assume we choose the hidden layer size as 100\n",
    "activation = 'Tanh'  # Choose 'Tanh' as the activation function\n",
    "\n",
    "# Create an instance of the Net_DGM model\n",
    "u = Net_DGM(dim_x=dim_x, dim_S=dim_S, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a981e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1.0  # Upper bound of the time interval\n",
    "N = 2000  # Number of sampling points\n",
    "\n",
    "# Randomly generate time points t ∈ [0, T)\n",
    "t_samples = torch.rand(N, 1) * T\n",
    "\n",
    "# Randomly generate spatial points x ∈ [-1, 1] x [-1, 1]\n",
    "x_samples = (torch.rand(N, 2) * 20) - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor([[0.001], [0.001]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "H = torch.tensor([[0.1, 0.0],\n",
    "                  [0.0, 0.1]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "M = torch.tensor([[1, 0],\n",
    "                  [0, 1]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "C = torch.tensor([[0.0, 0.0],\n",
    "                  [0.0, 0.0]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "D = torch.tensor([[1, 0],\n",
    "                  [0, 1]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "R = torch.tensor([[10.0, 0.0],\n",
    "                  [0.0, 10.0]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "a = torch.tensor([1.0, 1.0], dtype=torch.float32, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eccc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(model, t_samples, x_samples, sigma, H, M, C, D, R, alpha):\n",
    "    \"\"\"\n",
    "    Compute the total loss for the PDE problem, including the equation part and the boundary condition part.\n",
    "    \"\"\"\n",
    "    # Ensure input requires gradients\n",
    "    t_samples.requires_grad_(True)\n",
    "    x_samples.requires_grad_(True)\n",
    "\n",
    "    # model prediction\n",
    "    u = model(t_samples, x_samples)\n",
    "    print(type(u))\n",
    "    # # Time derivative\n",
    "    grad_u_t = torch.autograd.grad(u.sum(), t_samples, create_graph=True)[0]\n",
    "    \n",
    "    # Spatial derivative\n",
    "    grad_u_x = torch.autograd.grad(u.sum(), x_samples, create_graph=True, retain_graph=True)[0]\n",
    "    # Compute second derivatives for each dimension\n",
    "    hessians = []\n",
    "    for i in range(t_samples.size(0)):\n",
    "        def u_function(x_samples):\n",
    "            #Expand t and x to at least one-dimensional tensors to enable concatenation\n",
    "            t_expanded = t_samples[i].unsqueeze(0)\n",
    "            x_expanded = x_samples\n",
    "            return model(t_expanded, x_expanded).sum()\n",
    "\n",
    "        grad_u_xx = torch.autograd.functional.hessian(u_function, x_samples[i].unsqueeze(0))\n",
    "        hessians.append(grad_u_xx)\n",
    "\n",
    "    # use torch.squeeze()extra dimension，and collect processed tensors\n",
    "    squeezed_hessians = [torch.squeeze(tensor) for tensor in hessians]\n",
    "    \n",
    "    # Stack processed tensors using torch.stack()\n",
    "    stacked_hessians = torch.stack(squeezed_hessians)\n",
    "    grad_u_xx = stacked_hessians\n",
    "    \n",
    "    # calculate σσ^T\n",
    "    sigma_sigma_T = sigma @ sigma.T\n",
    "    # Assume n is the size of the first dimension of stacked_hessians\n",
    "    n = stacked_hessians.size(0)\n",
    "\n",
    "    # Add a new first dimension to sigma_sigma_T to make its shape [1, 2, 2]\n",
    "    sigma_sigma_T_unsqueezed = sigma_sigma_T.unsqueeze(0)\n",
    "\n",
    "    # Expand the size of the first dimension to n using the expand method to make its shape [n, 2, 2]\n",
    "    expanded_sigma_sigma_T = sigma_sigma_T_unsqueezed.expand(n, -1, -1)\n",
    "    print(expanded_sigma_sigma_T.size(),grad_u_xx.size())\n",
    "    product = torch.matmul(expanded_sigma_sigma_T, grad_u_xx)\n",
    "\n",
    "    # Compute the sum of each matrix diagonal element, i.e., trace, the result's shape is [n, 1]\n",
    "    trace = torch.diagonal(product, dim1=-2, dim2=-1).sum(-1, keepdim=True)\n",
    "\n",
    "    # Final diffusive term, multiply by 0.5\n",
    "    diffusive_term = 0.5 * trace\n",
    "    print(diffusive_term.size)\n",
    "    convective_term = torch.sum(grad_u_x * (H @ x_samples.T).T, dim=1, keepdim=True) + torch.sum(grad_u_x * (M @ alpha.unsqueeze(1)).T, dim=1, keepdim=True)\n",
    "    quadratic_term = torch.sum(x_samples * (C @ x_samples.T).T, dim=1, keepdim=True)\n",
    "    constant_term = alpha @ D @ alpha\n",
    "    print(grad_u_t.size(),diffusive_term.size(),convective_term.size(),quadratic_term.size())\n",
    "    pde_residual = grad_u_t + diffusive_term + convective_term + quadratic_term + constant_term\n",
    "    \n",
    "    # Loss for the equation part\n",
    "    loss_eqn = pde_residual.pow(2).mean()\n",
    "    \n",
    "    # Loss for the boundary condition part\n",
    "    T = torch.tensor(1.0, requires_grad=False).expand_as(t_samples)  # 假设边界在T=1时\n",
    "    u_T = model(T, x_samples)\n",
    "    boundary_condition = (u_T - torch.sum(x_samples * (R @ x_samples.T).T, dim=1, keepdim=True)).pow(2)\n",
    "    loss_boundary = boundary_condition.mean()\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = loss_eqn + loss_boundary\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MC simulation\n",
    "class LQR:\n",
    "    def __init__(self, H, M, C, D, R, T,sigma):\n",
    "        self.H = torch.tensor(H, dtype=torch.float32)\n",
    "        self.M = torch.tensor(M, dtype=torch.float32)\n",
    "        self.C = torch.tensor(C, dtype=torch.float32)\n",
    "        self.D = torch.tensor(D, dtype=torch.float32)\n",
    "        self.R = torch.tensor(R, dtype=torch.float32)\n",
    "        self.T = T\n",
    "        self.sigma = sigma \n",
    "\n",
    "    def solve_ricatti_ode(self, time_grid):\n",
    "        H, M, C, D, R = self.H.numpy(), self.M.numpy(), self.C.numpy(), self.D.numpy(), self.R.numpy()\n",
    "        T = self.T\n",
    "\n",
    "        def ricatti_ode(t, S_flat):\n",
    "            S = S_flat.reshape(2, 2)\n",
    "            dSdt = -2 * H.T @ S + S @ M @ np.linalg.inv(D) @ M.T @ S - C\n",
    "            return dSdt.flatten()\n",
    "        sol = solve_ivp(ricatti_ode, [T, 0], R.flatten(), t_eval=np.flip(time_grid), vectorized=True,rtol=1e-6, atol=1e-9)\n",
    "        S_values = sol.y.T.reshape(-1, 2, 2)\n",
    "        S_values_tensor = torch.tensor(S_values, dtype=torch.float32)\n",
    "        S_values_tensor_reversed = torch.flip(S_values_tensor, dims=[0])\n",
    "        return S_values_tensor_reversed\n",
    "    \n",
    "    def control_problem_value(self, t, x):\n",
    "        S_values = self.solve_ricatti_ode(t)\n",
    "        v_values = torch.zeros(x.size(0), 1, dtype=torch.float32)  # initialize v_values\n",
    "        # for cycle\n",
    "        for i in range(x.size(0)):\n",
    "            x_i = x[i]  # x_i shape as (1, 2)\n",
    "            S_i = S_values[i]  # S_i shape (2, 2)\n",
    "            v_i = torch.matmul(torch.matmul(x_i.transpose(0, 1), S_i), x_i)  # calculate x_i * S_i * x_i'\n",
    "\n",
    "            integral_value = 0  # initialize integral\n",
    "            if self.sigma is not None:  \n",
    "                sigma_mat = torch.tensor(self.sigma, dtype=torch.float32)\n",
    "                \n",
    "                for j in range(i, len(t) - 1):\n",
    "                    S_j = S_values[j]  \n",
    "                    tr_value = torch.trace(torch.matmul(torch.matmul(sigma_mat, sigma_mat.transpose(0, 1)), S_j))\n",
    "                    delta_t = t[j + 1] - t[j]  \n",
    "                    integral_value += tr_value * delta_t  \n",
    "\n",
    "            v_i += integral_value  \n",
    "            v_values[i] = v_i\n",
    "        return v_values\n",
    "    def markov_control_function(self, t, x):\n",
    "        S_values = self.solve_ricatti_ode(t)  \n",
    "       \n",
    "        a_values = [-torch.inverse(self.D) @ self.M.T @ S_values[i].clone().detach().to(dtype=torch.float32) @ x_[None, :] for i, x_ in enumerate(x)]\n",
    "        \n",
    "        a_values = torch.stack(a_values).squeeze(1)  \n",
    "        return a_values\n",
    "\n",
    "    \n",
    "def wiener_process(N,t,T):              \n",
    "    dW = np.random.normal(0, np.sqrt((T-t)/N))\n",
    "    return dW\n",
    "\n",
    "\n",
    "def simulation (lqr,x,t,T,N):\n",
    "    '''\n",
    "    this function simulate single path\n",
    "\n",
    "    '''\n",
    "    time_grid = torch.linspace(t, T, steps=N+1)\n",
    "    dt = (T-t)/N\n",
    "    x_values = torch.zeros((N+1, 2, 1))\n",
    "    x_values[0] = x\n",
    "    a = np.array([[1], [1]])\n",
    "    for n in range(N):\n",
    "        x_n = x_values[n]\n",
    "\n",
    "        dW =  wiener_process(N,t,T) \n",
    "        sigma_dW =  lqr.sigma * dW\n",
    "        x_next = x_n + dt * (lqr.H @ x_n +lqr.M.T@ a) + sigma_dW\n",
    "        x_values[n+1] = x_next\n",
    "    return x_values\n",
    "\n",
    "def simulate_multiple_paths(lqr, x,t, T, N, n):\n",
    "    '''\n",
    "    \n",
    "    t:initial time\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    all_simulations = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        simulation_result = simulation(lqr, x,t, T, N)\n",
    "        all_simulations.append(simulation_result)\n",
    "\n",
    "    \n",
    "    stacked_simulations = torch.stack(all_simulations)\n",
    "    \n",
    "    return stacked_simulations\n",
    "\n",
    "#%%\n",
    "def compute_J_alpha(lqr, X_paths,t,T, N,n):\n",
    "    C = lqr.C\n",
    "    D = lqr.D\n",
    "    R = lqr.R\n",
    "    dt = (T-t)/N\n",
    "    a = np.array([[1], [1]])\n",
    "    \n",
    "    a_tensor = torch.tensor(a, dtype=torch.float32)\n",
    "    a_expanded = a_tensor.unsqueeze(0).unsqueeze(0)  \n",
    "    a_expanded = a_expanded.expand(n, N+1, -1, -1)  \n",
    "    \n",
    "    print(a_expanded.size(),X_paths.size())\n",
    "    integral_part = torch.sum(X_paths.transpose(2, 3) @ C @ X_paths + a_expanded.transpose(2, 3) @ D @ a_expanded, dim=1) * dt\n",
    "    \n",
    "    terminal_cost = torch.sum(X_paths[:, -1].transpose(1, 2) @ R @ X_paths[:, -1], dim=1)\n",
    "\n",
    "    \n",
    "    J_alpha = torch.mean(integral_part + terminal_cost)\n",
    "    return J_alpha.item()\n",
    "#%%\n",
    "H = [[0.1, 0.0],\n",
    "     [0.0, 0.1]]\n",
    "M = [[1, 0],\n",
    "     [0, 1]]\n",
    "C = [[0.0, 0.0],\n",
    "     [0.0, 0.0]]\n",
    "D = [[1, 0],\n",
    "     [0, 1]]\n",
    "R = [[10.0, 0.0],\n",
    "     [0.0, 10.0]]\n",
    "T = 1\n",
    "t=0\n",
    "sigma =np.array([[0.001], [0.001]])\n",
    "N=100\n",
    "time_grid = np.linspace(t, T,N+1)\n",
    "\n",
    "initial_x = torch.tensor([[[0.0], [0.0]]], dtype=torch.float32)\n",
    "lqr = LQR(H=H, M=M, C=C, D=D, R=R, T=T,sigma=sigma)\n",
    "#%%\n",
    "n_simulate = simulate_multiple_paths(lqr, initial_x,t, T, N, n=100)\n",
    "J = compute_J_alpha(lqr, n_simulate,t,T, N,n=100)\n",
    "#%%\n",
    "# fix random seed to makesure same output\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# uniformly distributed (0,1)\n",
    "t_values = np.linspace(0, 1, 5)\n",
    "\n",
    "#Generate 100 sets of x values uniformly distributed, where each set of x values is within the range [-10, 10] x [-10, 10].\n",
    "x_values = np.random.uniform(-10, 10, (25, 2))\n",
    "x_values_all = []\n",
    "t_values_all = []\n",
    "J_values = []\n",
    "\n",
    "# for each t, use all x to calculate J\n",
    "for t in t_values:\n",
    "    for x in x_values:\n",
    "        n_simulate = simulate_multiple_paths(lqr, torch.tensor(x, dtype=torch.float32).reshape(1, 2, 1),t, T, N, n=100)\n",
    "        J = compute_J_alpha(lqr, n_simulate,t,T, N,n=100)\n",
    "        t_values_all.append(t)\n",
    "        x_values_all.append(x)\n",
    "        J_values.append(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa986f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 800  \n",
    "loss_all = []\n",
    "mse_all = []\n",
    "optimizer = torch.optim.Adam(u.parameters(), lr=0.008) \n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  \n",
    "    loss = compute_total_loss(u, t_samples, x_samples, sigma, H, M, C, D, R, a)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update parameters\n",
    "    loss_all.append(loss)\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        J_dgm = []\n",
    "        for t, x in zip(t_values_all, x_values_all):\n",
    "            # Assuming  model u takes time t and state x as input\n",
    "            #  Depending on the implementation details of  model u, we need to adjust the shape of the input here\n",
    "            x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "            t_tensor = torch.tensor([[t]], dtype=torch.float32)\n",
    "            J_pred = u(x_tensor, t_tensor)\n",
    "            J_dgm.append(J_pred.item())\n",
    "        mse_value = mse_loss(torch.tensor(J_dgm), torch.tensor(J_values))\n",
    "        mse_all.append(mse_value.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = [loss.item() for loss in loss_all]\n",
    "plt.loglog(range(1, len(loss_values) + 1), loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Log-Log Plot of Loss')\n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_all\n",
    "plt.loglog(range(1, len(mse_all) + 1), mse_all)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mse')\n",
    "plt.title('Log-Log Plot of Loss')\n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1492f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
