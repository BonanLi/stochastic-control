{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.integrate import solve_ivp\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGM_Layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(DGM_Layer, self).__init__()\n",
    "        \n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "            \n",
    "\n",
    "        self.gate_Z = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_G = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_R = self.layer(dim_x+dim_S, dim_S)\n",
    "        self.gate_H = self.layer(dim_x+dim_S, dim_S)\n",
    "            \n",
    "    def layer(self, nIn, nOut):\n",
    "        l = nn.Sequential(nn.Linear(nIn, nOut), self.activation)\n",
    "        return l\n",
    "    \n",
    "    def forward(self, x, S):\n",
    "        x_S = torch.cat([x,S],1)\n",
    "        Z = self.gate_Z(x_S)\n",
    "        G = self.gate_G(x_S)\n",
    "        R = self.gate_R(x_S)\n",
    "        \n",
    "        input_gate_H = torch.cat([x, S*R],1)\n",
    "        H = self.gate_H(input_gate_H)\n",
    "        \n",
    "        output = ((1-G))*H + Z*S\n",
    "        return output\n",
    "class Net_DGM(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_x, dim_S, activation='Tanh'):\n",
    "        super(Net_DGM, self).__init__()\n",
    "\n",
    "        self.dim = dim_x\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'LogSigmoid':\n",
    "            self.activation = nn.LogSigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function {}\".format(activation))\n",
    "\n",
    "        self.input_layer = nn.Sequential(nn.Linear(dim_x+1, dim_S), self.activation)\n",
    "\n",
    "        self.DGM1 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM2 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "        self.DGM3 = DGM_Layer(dim_x=dim_x+1, dim_S=dim_S, activation=activation)\n",
    "\n",
    "        self.output_layer = nn.Linear(dim_S, 1)\n",
    "\n",
    "    def forward(self,t,x):\n",
    "        tx = torch.cat([t,x], 1)\n",
    "        S1 = self.input_layer(tx)\n",
    "        S2 = self.DGM1(tx,S1)\n",
    "        S3 = self.DGM2(tx,S2)\n",
    "        S4 = self.DGM3(tx,S3)\n",
    "        output = self.output_layer(S4)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss_v(model_u, t_samples, x_samples, sigma, H, M, C, D, R, model_a):\n",
    "    \"\"\"\n",
    "    Compute the total loss for the PDE problem, including the equation part and the boundary condition part.\n",
    "    \"\"\"\n",
    "    # Ensure input requires gradients\n",
    "    t_samples.requires_grad_(True)\n",
    "    x_samples.requires_grad_(True)\n",
    "\n",
    "    # Model prediction\n",
    "    u = model_u(t_samples, x_samples)\n",
    "    \n",
    "    combined_input = torch.cat((t_samples, x_samples), dim=1)\n",
    "    alpha = model_a(combined_input)\n",
    "    \n",
    "    # Time derivative\n",
    "    grad_u_t = torch.autograd.grad(u.sum(), t_samples, create_graph=True)[0]\n",
    "    \n",
    "    # Spatial derivative\n",
    "    grad_u_x = torch.autograd.grad(u.sum(), x_samples, create_graph=True, retain_graph=True)[0]\n",
    "    \n",
    "    # Compute second derivatives for each dimension\n",
    "    hessians = []\n",
    "    for i in range(t_samples.size(0)):\n",
    "        def u_function(x_samples):\n",
    "            # Expand t and x to at least one-dimensional tensors to enable concatenation\n",
    "            t_expanded = t_samples[i].unsqueeze(0)\n",
    "            x_expanded = x_samples\n",
    "            return model_u(t_expanded, x_expanded).sum()\n",
    "        # Note: Assuming x_samples[i] is already at least one-dimensional tensor, no need for expansion\n",
    "        grad_u_xx = torch.autograd.functional.hessian(u_function, x_samples[i].unsqueeze(0))\n",
    "        hessians.append(grad_u_xx)\n",
    "\n",
    "    # Squeeze extra singleton dimensions and collect processed tensors\n",
    "    squeezed_hessians = [torch.squeeze(tensor) for tensor in hessians]\n",
    "    \n",
    "    # Stack processed tensors\n",
    "    stacked_hessians = torch.stack(squeezed_hessians)\n",
    "    grad_u_xx = stacked_hessians\n",
    "    \n",
    "    # Compute σσ^T\n",
    "    sigma_sigma_T = sigma @ sigma.T\n",
    "    n = stacked_hessians.size(0)\n",
    "\n",
    "    # Add a new first dimension to sigma_sigma_T\n",
    "    sigma_sigma_T_unsqueezed = sigma_sigma_T.unsqueeze(0)\n",
    "\n",
    "    # Expand the size of the first dimension to n\n",
    "    expanded_sigma_sigma_T = sigma_sigma_T_unsqueezed.expand(n, -1, -1)\n",
    "    \n",
    "    # Compute diffusion term\n",
    "    product = torch.matmul(expanded_sigma_sigma_T, grad_u_xx)\n",
    "    trace = torch.diagonal(product, dim1=-2, dim2=-1).sum(-1, keepdim=True)\n",
    "    diffusive_term = 0.5 * trace\n",
    "    \n",
    "    # Compute convective term\n",
    "    convective_term = torch.sum(grad_u_x * (H @ x_samples.T).T, dim=1, keepdim=True) + torch.sum(grad_u_x * (M @ alpha.T).T, dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute quadratic term\n",
    "    quadratic_term = torch.sum(x_samples * (C @ x_samples.T).T, dim=1, keepdim=True)\n",
    "   \n",
    "    # Compute constant term\n",
    "    temp = alpha @ D\n",
    "    constant_term = torch.einsum('ij,ij->i', temp, alpha)\n",
    "    constant_term = constant_term.unsqueeze(-1)\n",
    "    \n",
    "    # Compute PDE residual\n",
    "    pde_residual = grad_u_t + diffusive_term + convective_term + quadratic_term + constant_term\n",
    "    \n",
    "    # Loss for the equation part\n",
    "    loss_eqn = pde_residual.pow(2).mean()\n",
    "    \n",
    "    # Loss for the boundary condition part\n",
    "    T = torch.tensor(1.0, requires_grad=False).expand_as(t_samples)  # Assume boundary is at T=1\n",
    "    u_T = model_u(T, x_samples)\n",
    "    boundary_condition = (u_T - torch.sum(x_samples * (R @ x_samples.T).T, dim=1, keepdim=True)).pow(2)\n",
    "    loss_boundary = boundary_condition.mean()\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = loss_eqn + loss_boundary\n",
    "    \n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing u\n",
    "dim_x = 2  # 1-dimensional time + 2-dimensional space\n",
    "dim_S = 100  # Assume the hidden layer size is 100\n",
    "activation = 'Tanh'  # Select 'Tanh' as the activation function\n",
    "\n",
    "# Creating an instance of the Net_DGM model\n",
    "v = Net_DGM(dim_x=dim_x, dim_S=dim_S, activation=activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1.0  # Upper bound of the time interval\n",
    "N = 2000  # Number of sampling points\n",
    "\n",
    "# Randomly generate time points t ∈ [0, T)\n",
    "t_samples = torch.rand(N, 1) * T\n",
    "\n",
    "# Randomly generate spatial points x ∈ [-1, 1] x [-1, 1]\n",
    "x_samples = (torch.rand(N, 2) * 20) - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a080abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = torch.tensor([[0.001], [0.001]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "H = torch.tensor([[0.1, 0.0],\n",
    "                  [0.0, 0.1]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "M = torch.tensor([[1, 0],\n",
    "                  [0, 1]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "C = torch.tensor([[0.0, 0.0],\n",
    "                  [0.0, 0.0]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "D = torch.tensor([[1, 0],\n",
    "                  [0, 1]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "R = torch.tensor([[10.0, 0.0],\n",
    "                  [0.0, 10.0]], dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "a = torch.tensor([1.0, 1.0], dtype=torch.float32, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss_a(model_u, t_samples, x_samples, sigma, H, M, C, D, R, model_a):\n",
    "    \"\"\"\n",
    "    Compute the total loss for the PDE problem, including the equation part and the boundary condition part.\n",
    "    \"\"\"\n",
    "    # Ensure input requires gradients\n",
    "    t_samples.requires_grad_(True)\n",
    "    x_samples.requires_grad_(True)\n",
    "\n",
    "    # Model prediction\n",
    "    u = model_u(t_samples, x_samples)\n",
    "    combined_input = torch.cat((t_samples, x_samples), dim=1)\n",
    "    alpha = model_a(combined_input)\n",
    "\n",
    "    # Spatial derivative\n",
    "    grad_u_x = torch.autograd.grad(u.sum(), x_samples, create_graph=True, retain_graph=True)[0]\n",
    "    \n",
    "    # Compute convective term\n",
    "    convective_term = torch.sum(grad_u_x * (H @ x_samples.T).T, dim=1, keepdim=True) + torch.sum(grad_u_x * (M @ alpha.T).T, dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute quadratic term\n",
    "    quadratic_term = torch.sum(x_samples * (C @ x_samples.T).T, dim=1, keepdim=True)\n",
    "   \n",
    "    # Compute constant term\n",
    "    temp = alpha @ D\n",
    "    constant_term = torch.einsum('ij,ij->i', temp, alpha)\n",
    "    constant_term = constant_term.unsqueeze(-1)\n",
    "    \n",
    "    # Compute PDE residual\n",
    "    pde_residual = convective_term + quadratic_term + constant_term\n",
    "    \n",
    "    # Loss for the equation part\n",
    "    loss_eqn = pde_residual.mean()\n",
    "    \n",
    "    return loss_eqn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed867970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "\n",
    "    def __init__(self, sizes, activation=nn.ReLU, output_activation=nn.Identity, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.BatchNorm1d(sizes[0]),] if batch_norm else []\n",
    "        for j in range(len(sizes)-1):\n",
    "            layers.append(nn.Linear(sizes[j], sizes[j+1]))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(sizes[j+1], affine=True))\n",
    "            if j<(len(sizes)-2):\n",
    "                layers.append(activation())\n",
    "            else:\n",
    "                layers.append(output_activation())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def freeze(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe116252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing u\n",
    "dim_x_v = 2  # Adjust according to the actual situation\n",
    "dim_S_v = 100  # Assume the hidden layer size is 100\n",
    "activation_v = 'Tanh'  # Select 'Tanh' as the activation function\n",
    "\n",
    "# Creating an instance of the Net_DGM model\n",
    "v = Net_DGM(dim_x=dim_x_v, dim_S=dim_S_v, activation=activation_v)\n",
    "optimizer = torch.optim.Adam(v.parameters(), lr=0.01) \n",
    "\n",
    "# Establishing a\n",
    "dim_x_a = 3  # Adjust according to the actual situation\n",
    "output_dim_a = 2\n",
    "hidden_layers_a = [100, 100]  # Increase more hidden layers if necessary, e.g., [100, 100] indicates two hidden layers with 100 nodes each\n",
    "\n",
    "# Define network structure for a\n",
    "a = FFN([dim_x_a] + hidden_layers_a + [output_dim_a], activation=nn.ReLU, output_activation=nn.Identity)\n",
    "optimizer2 = torch.optim.Adam(a.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93500590",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_v = 12649.0303\n",
    "t = torch.tensor([0.], dtype=torch.float32).unsqueeze(0)\n",
    "x = torch.tensor([100.0,50.0], dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399d217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 30\n",
    "e_all = []\n",
    "v_all = []\n",
    "\n",
    "for i in range(m):\n",
    "    # Train v first\n",
    "    # Training the first one\n",
    "    num_epochs_v = 5  # Number of epochs for v\n",
    "    optimizer_v = torch.optim.Adam(v.parameters(), lr=0.01) \n",
    "    for epoch in range(num_epochs_v):\n",
    "        optimizer_v.zero_grad()  # Clear gradients at the beginning of each epoch\n",
    "        loss_v = compute_total_loss_v(v, t_samples, x_samples, sigma, H, M, C, D, R, a)  # Compute loss\n",
    "        loss_v.backward()  # Backpropagation\n",
    "        optimizer_v.step()  # Update parameters\n",
    "        print(f\"Epoch_v {epoch}, Loss: {loss_v.item()}\")\n",
    "\n",
    "    # Training the second one\n",
    "    num_epochs_a = 5  # Number of epochs for a\n",
    "    optimizer_a = torch.optim.Adam(a.parameters(), lr=0.001)\n",
    "    for epoch in range(num_epochs_a):\n",
    "        optimizer_a.zero_grad()  # Clear gradients\n",
    "        loss_a = compute_total_loss_a(v, t_samples, x_samples, sigma, H, M, C, D, R, a)  # Compute loss\n",
    "        loss_a.backward()  # Backpropagation\n",
    "        optimizer_a.step()  # Update parameters\n",
    "        print(f\"Epoch_a {epoch}, Loss: {loss_a.item()}\")\n",
    "\n",
    "    e = abs(true_v - v(t, x))\n",
    "    e_all.append(e)\n",
    "    v_all.append(v(t, x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded51fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_all \n",
    "e_values = [e.item() for e in e_all]\n",
    "e_values_30= e_values[:30]\n",
    "plt.loglog(range(1, len(e_values_30) + 1), e_values_30)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Log-Log Plot of Error')\n",
    "plt.grid(True)  \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
